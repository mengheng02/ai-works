{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"42z1SQqKFdtg"},"source":["# Lab 4. Probabilistic Inference\n","# Task 4.2 Diabetes Diagnosis Using Naïve Bayes\n","## Problem Descriptions\n","In this task, we implement the Gaussian Naive Bayes model to classify diabetes patients into 3 classes (Class 1: Progression measure 0-150,\n","Class 2: Progression measure 150-300, Class 3: Progression measure 300+).The diabetes diagnosis problem can be formulated as follow.\n","\n","\n","---\n","Assuming conditional independence between 10 features (f1=age, f2=sex, f3=bmi, f4=average blood pressure, f5=total serum cholesterol, f6=low-density lipoproteins, f7=high-density lipoproteins, f8=total cholesterol / HDL, f9=possibly log of serum triglycerides level, f10=blood sugar level) given 3 progression measure classes (c1=0-150, c2=150-300, c3=300+).\n","\n","    𝑃(𝑐𝑖|𝑓1, 𝑓2, ..., f10) = 𝛼𝑃(𝑓1|𝑐𝑖) 𝑃(𝑓2|𝑐𝑖) ... P(𝑓10|𝑐𝑖) 𝑃(𝑐𝑖)\n","\n","* 𝑐𝑖: progression measure class, where i=1,2,3.\n","\n","*  𝑃(𝑐𝑖|𝑓1, 𝑓2, ..., f10): Posterior probability of progression measure class ci given f1, f2, f3, f4, f5, f6, f7 ,f8, f9, and f10 features.\n","* 𝑃(𝑓1|𝑐𝑖): Conditional probability of f1 occuring given ci has   \n","  occured.\n","*  𝑃(𝑓2|𝑐𝑖): Conditional probability of f2 occuring given ci has occured.\n","*  P(ci): Probability of progression measure class ci occurs.\n","\n","We need to identify the prior probability 𝑃(𝑐𝑖) of each progression measure class and the conditional probabilities 𝑃(𝑓1|𝑐𝑖), 𝑃(𝑓2|𝑐𝑖), 𝑃(𝑓3|𝑐𝑖) until P(𝑓10|𝑐𝑖) first in order to estimate the posterior probability of progression meassure class given  features 𝑃(𝑐𝑖| 𝑓1, 𝑓2,𝑓3, 𝑓4, ..., f10) where i=1,2,3 for the three progression measure classes.\n","\n","\n","\n","\n","## Implementation and Results"]},{"cell_type":"code","metadata":{"id":"y6A3ZHOwFOPk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703719076322,"user_tz":0,"elapsed":3536,"user":{"displayName":"mh chong","userId":"16669753525538908106"}},"outputId":"c771b863-819d-48f6-b893-d4c98fa9a41a"},"source":["!pip install sklearn\n","from sklearn import datasets\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split, cross_validate\n","\n","# import numpy as np\n","import matplotlib.pyplot as plt\n","# from matplotlib import patches\n","import math"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sklearn\n","  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}]},{"cell_type":"code","metadata":{"id":"qmR8h6iHGXQJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703719082673,"user_tz":0,"elapsed":167,"user":{"displayName":"mh chong","userId":"16669753525538908106"}},"outputId":"45a6d278-e0da-4f4b-a45c-3cb0862c42d2"},"source":["diabetes = datasets.load_diabetes()\n","# X = diabetes.data[:,[2,3,9]]\n","X = diabetes.data\n","Y = [math.floor(x/150) for x in diabetes.target]\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n","\n","nb = GaussianNB()\n","nb.fit(X_train, Y_train)\n","Y_pred = nb.predict(X_test)\n","acc = accuracy_score(Y_test, Y_pred)\n","cm = confusion_matrix(Y_test, Y_pred)\n","cr = classification_report(Y_test, Y_pred)\n","\n","print(\"Accuracy:\", acc)\n","print(\"Confusion Matrix:\\n\", cm)\n","print(\"Prior:\\n\", nb.class_prior_)\n","print(\"Mean:\\n\", nb.theta_)\n","print(\"Variance:\\n\", nb.var_)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.7117117117117117\n","Confusion Matrix:\n"," [[42 17  0]\n"," [ 9 36  3]\n"," [ 0  3  1]]\n","Prior:\n"," [0.5407855  0.42900302 0.03021148]\n","Mean:\n"," [[-0.0078686  -0.00203973 -0.01786921 -0.01664023 -0.00848716 -0.00598924\n","   0.01384965 -0.01667859 -0.02296223 -0.0133189 ]\n"," [ 0.00594585 -0.00369356  0.02146801  0.01849403  0.00803364  0.00581356\n","  -0.01573671  0.01659892  0.02441361  0.01260292]\n"," [ 0.0042933   0.01255142  0.06536077  0.03564379  0.00503561 -0.00125125\n","  -0.02646531  0.02545259  0.04148047  0.04655653]]\n","Variance:\n"," [[0.00217604 0.00224597 0.00154143 0.00162569 0.00190696 0.00194978\n","  0.00204143 0.00157131 0.00151421 0.00175995]\n"," [0.0020812  0.0022265  0.00212051 0.00238253 0.00233841 0.00233336\n","  0.00153695 0.00223229 0.00184442 0.00234737]\n"," [0.00265239 0.0021807  0.00137571 0.0024228  0.00076896 0.00095186\n","  0.00106592 0.00206759 0.00127187 0.00179549]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"q8XVMr4NFy8v"},"source":["## Discussions\n","\n","In this task, the Gaussian Naive Bayes model is trained and tested. The performance is evaluated in terms of model accuracy, and confusion matrix. The results are collected in terms of a prior probability for each class, the mean value of each feature for each class, and the variance of each feature for each class.\n","\n","The Naive Bayes classifier offers a fairly good accuracy of 71.17%. It correctly classifies 42 instances from class 1, but 17 instances of class 1 are incorrectly predicted as class 2. For class 2, it successfully classifies 36 instances but incorrectly predicted 9 instances as belonging to class 1 and 3 instances as belonging to class 3. For class 3, it correctly predicted 1 instance, but 3 instances of class 3 were incorrectly predicted as class 2.\n","\n","From the prior probability, we can infer that class 1 makes up 54.08% of the data, class 2 makes up 42.9% of the data, and class 3 makes up 3.02% of the data. The mean values of each feature are close across different classes, and the variance of each class feature is small too. It means that the feature data points for each class are concentrated around the mean, increasing the model's difficulty in differentiating them. Hence, there is a risk that the model might not generalize well to new, unseen data.\n","\n","In conclusion, the classifier is fairly good at classifying patients into class 1 and class 2. There is improvement required for class 3 prediction. It might be due to the small sample size of class 3, as the patient with 300+ progression measure is very rare. The imbalanced dataset is likely to affect the classifier's performance. Also, class 3 seems like not being well-separated from class 2 in feature space, but well separated from class 1.\n"]}]}